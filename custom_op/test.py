import paddle
from custom_setup_op_test import *

def string_to_callable(func_path):
    """
    æ”¯æŒæ›´å¤æ‚çš„æ¨¡å—è·¯å¾„ï¼Œå¦‚ "paddle.nn.functional.relu" , "custom_setup_op_test.custom_relu"
    """
    if func_path is None:
        return None
    parts = func_path.split('.')
    
    # å°è¯•é€çº§å¯¼å…¥
    for i in range(len(parts)-1, 0, -1):
        module_path = '.'.join(parts[:i])
        func_name = parts[i]
        
        try:
            module = importlib.import_module(module_path)
            func = getattr(module, func_name)
            
            # æ£€æŸ¥å‰©ä½™éƒ¨åˆ†æ˜¯å¦æ˜¯å±æ€§
            for attr_name in parts[i+1:]:
                func = getattr(func, attr_name)
                
            return func
        except (ImportError, AttributeError):
            continue
    
    raise ValueError(f"æ— æ³•è§£æå‡½æ•°è·¯å¾„: {func_path}")

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="CUDAç®—å­æ€§èƒ½å¯¹æ¯”åŸºå‡†æµ‹è¯•",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python benchmark.py custom_relu paddle.relu
  python benchmark.py my_ops.custom_conv paddle.nn.functional.conv2d --input-shapes 1,64,224,224 1,128,112,112
  python benchmark.py torch.relu paddle.relu --warmup 5 --runs 50 --output report.xlsx
        """
    )
    
    parser.add_argument('custom_op', default=None,help='è‡ªå®šä¹‰ç®—å­è·¯å¾„ (å¦‚: custom_ops.my_relu)')
    parser.add_argument('reference_op', default=None,help='å‚è€ƒç®—å­è·¯å¾„ (å¦‚: paddle.relu, torch.relu)')
    
    parser.add_argument('--input-shapes', nargs='+', 
                       default=['1024', '2048', '4096', '8192'],
                       help='è¾“å…¥å½¢çŠ¶åˆ—è¡¨ (é»˜è®¤: 1024 2048 4096 8192)')
    
    parser.add_argument('--warmup', type=int, default=10,
                       help='é¢„çƒ­è¿è¡Œæ¬¡æ•° (é»˜è®¤: 10)')
    
    parser.add_argument('--runs', type=int, default=100,
                       help='åŸºå‡†æµ‹è¯•è¿è¡Œæ¬¡æ•° (é»˜è®¤: 100)')
    
    parser.add_argument('--output', type=str, 
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (Excelæ ¼å¼)')
    
    parser.add_argument('--plot', type=str,
                       help='ä¿å­˜å›¾è¡¨æ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--check_correctness', type=bool,
                        default=False,
                       help='ä¿å­˜å›¾è¡¨æ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--device', choices=['cuda', 'cpu'], default='cuda',
                       help='è¿è¡Œè®¾å¤‡ (é»˜è®¤: cuda)')
    
    return parser.parse_args()

def benchmark_op(op_name,reference_op_name,input_shapes,config):
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = BenchmarkRunner()
    
    # åŸºç¡€é…ç½®
    test_op_func = string_to_callable(op_name)
    reference_op_func = string_to_callable(reference_op_name)
    if reference_op_func is None:
        # æµ‹è¯•å•ä¸ªç®—å­
        print("ğŸ§ª æµ‹è¯•å•ä¸ªCUDAç®—å­")
        result = runner.benchmark.benchmark_operator(
            op_func=your_custom_op,
            input_shapes=input_shapes,  
            config=config,
            reference_func=reference_op_func,
            op_name=op_name
        )
    
    # # è¿è¡Œç¼©æ”¾æµ‹è¯•
    # scaling_results = runner.run_scaling_test(
    #     op_func=your_custom_op,
    #     base_shapes=[(256, 256), (256, 256)],
    #     scale_factors=[0.5, 1.0, 2.0, 4.0],
    #     op_name="my_custom_cuda_op"
    # )
    # æ¯”è¾ƒå¤šä¸ªç®—å­ï¼ˆå¦‚æœæœ‰å¤šä¸ªå®ç°ï¼‰
    operators_to_compare = {
        op_name : test_op_func,
        reference_op_name : reference_op_func, 
    }

    comparison = runner.compare_operators(
        operators=operators_to_compare,
        input_shapes=input_shapes,
        config=config
    )
    
    # # ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨
    # runner.generate_report("benchmark_report.csv")
    # runner.plot_results("benchmark_results.png")

def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPUæ¨¡å¼")
        args.device = 'cpu'
    
    print(f"ğŸš€ å¼€å§‹åŸºå‡†æµ‹è¯•: {args.custom_op} vs {args.reference_op}")
    print(f"ğŸ“Ÿ è®¾å¤‡: {args.device}")
    print(f"ğŸ“Š è¾“å…¥å½¢çŠ¶: {args.input_shapes}")
    print(f"âš™ï¸  é¢„çƒ­: {args.warmup} æ¬¡, æµ‹è¯•: {args.runs} æ¬¡")
    
    # è§£æè¾“å…¥å½¢çŠ¶
    input_shapes = []
    for shape_str in args.input_shapes:
        try:
            if ',' in shape_str:
                # å¤šç»´å½¢çŠ¶ï¼Œå¦‚ "1,64,224,224"
                shape = tuple(map(int, shape_str.split(',')))
            else:
                # ä¸€ç»´å½¢çŠ¶ï¼Œå¦‚ "1024"
                shape = (int(shape_str),)
            input_shapes.append(shape)

    config = BenchmarkConfig(
        warmup_runs=args.warmup,
        benchmark_runs=args.runs,
        sync_cuda=True,
        check_correctness=args.check_correctness,
        memory_stats=False
    )
    benchmark_op(args.custom_op,args.reference_op,input_shapes,config)