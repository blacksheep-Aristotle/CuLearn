from log import logger
from timer import get_timers
import paddle
import time
import numpy as np
from typing import Dict, List, Callable, Optional
import warnings
import gc
import matplotlib.pyplot as plt
import pandas as pd
from dataclasses import dataclass

@dataclass
class BenchmarkConfig:
    """åŸºå‡†æµ‹è¯•é…ç½®"""
    warmup_runs: int = 10
    benchmark_runs: int = 100
    sync_cuda: bool = True
    check_correctness: bool = True
    memory_stats: bool = True

class CUDABenchmark:
    """CUDAç®—å­åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self, device: str = "cuda", dtype: paddle.dtype = paddle.float32):
        self.device = device
        self.dtype = dtype
        self.results = {}
        self.timers = get_timers()
        
    def _sync_if_needed(self):
        """å¦‚æœéœ€è¦ï¼ŒåŒæ­¥CUDAè®¾å¤‡"""
        if paddle.cuda.is_available():
            paddle.cuda.synchronize(self.device)
    
    def _generate_test_inputs(self, input_shapes, low=-1.0, high=1.0):
        """ç”Ÿæˆæµ‹è¯•è¾“å…¥å¼ é‡"""
        inputs = []
        for shape in input_shapes:
            if isinstance(shape, tuple):
                tensor = paddle.rand(*shape, device=self.device, dtype=self.dtype) * (high - low) + low
            else:
                # å‡è®¾æ˜¯å•ä¸ªæ•´æ•°ï¼Œåˆ›å»º1Då¼ é‡
                tensor = paddle.rand(shape, device=self.device, dtype=self.dtype) * (high - low) + low
            inputs.append(tensor)
        return inputs
    
    def _get_memory_stats(self):
        """è·å–å†…å­˜ç»Ÿè®¡"""
        if paddle.cuda.is_available():
            return {
                'allocated': paddle.cuda.memory_allocated(self.device),
                'reserved': paddle.cuda.memory_reserved(self.device),
                'max_allocated': paddle.cuda.max_memory_allocated(self.device)
            }
        return {}
    
    def benchmark_operator(
        self,
        op_func: Callable,
        input_shapes: List,
        config: BenchmarkConfig = BenchmarkConfig(),
        reference_func: Optional[Callable] = None,
        op_name: str = "custom_op"
    ) -> Dict:
        """
        åŸºå‡†æµ‹è¯•å•ä¸ªç®—å­
        
        Args:
            op_func: è¦æµ‹è¯•çš„ç®—å­å‡½æ•°
            input_shapes: è¾“å…¥å¼ é‡å½¢çŠ¶åˆ—è¡¨
            config: åŸºå‡†æµ‹è¯•é…ç½®
            reference_func: å‚è€ƒå®ç°ï¼ˆç”¨äºæ­£ç¡®æ€§æ£€æŸ¥ï¼‰
            op_name: ç®—å­åç§°
            
        Returns:
            åŒ…å«æ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        print(f"ğŸƒâ€â™‚ï¸ å¼€å§‹åŸºå‡†æµ‹è¯•: {op_name}")
        
        # æ¸…ç†å†…å­˜
        if paddle.cuda.is_available():
            paddle.cuda.empty_cache()
        
        # ç”Ÿæˆæµ‹è¯•è¾“å…¥
        inputs = self._generate_test_inputs(input_shapes)
        
        # é¢„çƒ­è¿è¡Œ
        print(f"  ğŸ”¥ é¢„çƒ­è¿è¡Œ ({config.warmup_runs} æ¬¡)...")
        for _ in range(config.warmup_runs):
            outputs = op_func(*inputs)
            if config.sync_cuda:
                self._sync_if_needed()
        
        # æ­£ç¡®æ€§æ£€æŸ¥ï¼ˆå¦‚æœæœ‰å‚è€ƒå®ç°ï¼‰
        correctness_passed = True
        if config.check_correctness and reference_func is not None:
            print("  âœ… è¿è¡Œæ­£ç¡®æ€§æ£€æŸ¥...")
            try:
                ref_outputs = reference_func(*[x.cpu() for x in inputs])
                test_outputs = op_func(*inputs)
                if config.sync_cuda:
                    self._sync_if_needed()
                
                # è½¬æ¢åˆ°CPUæ¯”è¾ƒ
                test_outputs_cpu = test_outputs.cpu() if test_outputs.is_cuda else test_outputs
                
                # æ£€æŸ¥æ•°å€¼æ¥è¿‘ç¨‹åº¦
                if isinstance(test_outputs_cpu, torch.Tensor):
                    diff = torch.abs(test_outputs_cpu - ref_outputs)
                    max_diff = diff.max().item()
                    avg_diff = diff.mean().item()
                    
                    correctness_passed = max_diff < 1e-4
                    print(f"    æœ€å¤§å·®å¼‚: {max_diff:.6f}, å¹³å‡å·®å¼‚: {avg_diff:.6f}")
                    
                    if not correctness_passed:
                        warnings.warn(f"æ­£ç¡®æ€§æ£€æŸ¥å¤±è´¥! æœ€å¤§å·®å¼‚: {max_diff}")
            except Exception as e:
                print(f"     âš ï¸ æ­£ç¡®æ€§æ£€æŸ¥å‡ºé”™: {e}")
                correctness_passed = False
        
        # åŸºå‡†æµ‹è¯•è¿è¡Œ
        print(f"   â±ï¸ åŸºå‡†æµ‹è¯•è¿è¡Œ ({config.benchmark_runs} æ¬¡)...")
        timings = []
        memory_stats = []
        
        for i in range(config.benchmark_runs):
            # è®°å½•å†…å­˜ä½¿ç”¨å‰çŠ¶æ€
            if config.memory_stats:
                memory_before = self._get_memory_stats()
            
            # è®¡æ—¶å¼€å§‹
            self.timers("read-data").start()
            
            # è¿è¡Œç®—å­
            outputs = op_func(*inputs)
            
            # åŒæ­¥è®¾å¤‡
            if config.sync_cuda:
                self._sync_if_needed()
            
            # è®¡æ—¶ç»“æŸ
            self.timers("read-data").stop()
            
            # è®°å½•å†…å­˜ä½¿ç”¨åçŠ¶æ€
            if config.memory_stats:
                memory_after = self._get_memory_stats()
                memory_stats.append({
                    'allocated_diff': memory_after['allocated'] - memory_before['allocated'],
                    'iteration': i
                })
        
        # ç»Ÿè®¡åˆ†æ
        timer_info = self.timers.log(self.timers.timers.keys(), reset=True)
        
        if config.memory_stats and memory_stats:
            memory_diffs = [m['allocated_diff'] for m in memory_stats]
            stats['memory_allocated_avg'] = np.mean(memory_diffs)
            stats['memory_allocated_max'] = np.max(memory_diffs)
            stats['timer_info'] = timer_info
        
        self.results[op_name] = stats

        return stats

class BenchmarkRunner:
    """æ‰¹é‡åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.benchmark = CUDABenchmark()
        self.comparison_results = []
    
    def run_scaling_test(
        self,
        op_func: Callable,
        base_shapes: List,
        scale_factors: List[float],
        op_name: str = "custom_op"
    ):
        """è¿è¡Œç¼©æ”¾æµ‹è¯•ï¼ˆä¸åŒè¾“å…¥å¤§å°ï¼‰"""
        print(f"\nğŸ“ˆ è¿è¡Œç¼©æ”¾æµ‹è¯•: {op_name}")
        print("=" * 60)
        
        scaling_results = []
        
        for scale in scale_factors:
            # ç¼©æ”¾è¾“å…¥å½¢çŠ¶
            scaled_shapes = []
            for shape in base_shapes:
                if isinstance(shape, tuple):
                    scaled_shape = tuple(int(dim * scale) for dim in shape)
                else:
                    scaled_shape = int(shape * scale)
                scaled_shapes.append(scaled_shape)
            
            print(f"  ç¼©æ”¾å› å­: {scale:.1f}, è¾“å…¥å½¢çŠ¶: {scaled_shapes}")
            
            # è¿è¡ŒåŸºå‡†æµ‹è¯•
            result = self.benchmark.benchmark_operator(
                op_func, scaled_shapes, op_name=f"{op_name}_scale_{scale}"
            )
            
            scaling_results.append({
                'scale_factor': scale,
                'input_shapes': scaled_shapes,
                **result
            })
        
        return scaling_results
    
    def compare_operators(
        self,
        operators: Dict[str, Callable],
        input_shapes: List,
        config: BenchmarkConfig = BenchmarkConfig()
    ):
        """æ¯”è¾ƒå¤šä¸ªç®—å­çš„æ€§èƒ½"""
        print(f"\nâš–ï¸ æ¯”è¾ƒç®—å­æ€§èƒ½")
        print("=" * 60)
        
        comparison = []
        
        for op_name, op_func in operators.items():
            print(f"\næµ‹è¯•ç®—å­: {op_name}")
            result = self.benchmark.benchmark_operator(
                op_func, input_shapes, config=config, op_name=op_name
            )
            comparison.append(result)
            
            # æ‰“å°ç®€è¦ç»“æœ
            logger.info(f"Profile op_name {op_name} : {result}")
        
        self.comparison_results = comparison
        return comparison

    def generate_report(self, filename: Optional[str] = None):
        """ç”Ÿæˆè¯¦ç»†æµ‹è¯•æŠ¥å‘Š"""
        if not self.benchmark.results:
            print("æ²¡æœ‰æµ‹è¯•ç»“æœå¯æŠ¥å‘Š")
            return
        
        print("\n" + "="*80)
        print("ğŸ“Š åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        # åˆ›å»ºæ•°æ®æ¡†ç”¨äºæ˜¾ç¤º
        report_data = []
        for op_name, result in self.benchmark.results.items():
            report_data.append({
                'Operator': op_name,
                'Input Shapes': str(result['input_shapes']),
                'Mean (ms)': f"{result['mean_ms']:.4f}",
                'Std (ms)': f"{result['std_ms']:.4f}",
                'Min (ms)': f"{result['min_ms']:.4f}",
                'Max (ms)': f"{result['max_ms']:.4f}",
                'Throughput (ops/sec)': f"{result['throughput']:.2f}",
                'P95 (ms)': f"{result['p95_ms']:.4f}",
                'Correctness': 'âœ…' if result.get('correctness_passed', True) else 'âŒ'
            })
        
        df = pd.DataFrame(report_data)
        print(df.to_string(index=False))
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if filename:
            df.to_csv(filename, index=False)
            print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
        
        return df
    
    def plot_results(self, save_path: Optional[str] = None):
        """ç»˜åˆ¶æµ‹è¯•ç»“æœå›¾è¡¨"""
        if not self.benchmark.results:
            print("æ²¡æœ‰æµ‹è¯•ç»“æœå¯ç»˜åˆ¶")
            return
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. å¹³å‡æ—¶é—´æ¯”è¾ƒ
        operators = list(self.benchmark.results.keys())
        means = [self.benchmark.results[op]['mean_ms'] for op in operators]
        
        bars = ax1.bar(operators, means)
        ax1.set_title('å¹³å‡æ‰§è¡Œæ—¶é—´æ¯”è¾ƒ')
        ax1.set_ylabel('æ—¶é—´ (ms)')
        ax1.bar_label(bars, fmt='%.3f')
        
        # 2. ååé‡æ¯”è¾ƒ
        throughputs = [self.benchmark.results[op]['throughput'] for op in operators]
        bars = ax2.bar(operators, throughputs)
        ax2.set_title('ååé‡æ¯”è¾ƒ')
        ax2.set_ylabel('æ“ä½œ/ç§’')
        ax2.bar_label(bars, fmt='%.0f')
        
        # 3. æ—¶é—´åˆ†å¸ƒï¼ˆç®±çº¿å›¾ï¼‰
        timing_data = [self.benchmark.results[op]['timings_ms'] for op in operators]
        ax3.boxplot(timing_data, labels=operators)
        ax3.set_title('æ‰§è¡Œæ—¶é—´åˆ†å¸ƒ')
        ax3.set_ylabel('æ—¶é—´ (ms)')
        
        # 4. ç™¾åˆ†ä½æ•°æ¯”è¾ƒ
        percentiles_data = {
            'P50': [self.benchmark.results[op]['p50_ms'] for op in operators],
            'P95': [self.benchmark.results[op]['p95_ms'] for op in operators],
            'P99': [self.benchmark.results[op]['p99_ms'] for op in operators]
        }
        
        x = np.arange(len(operators))
        width = 0.25
        multiplier = 0
        
        for attr, measurement in percentiles_data.items():
            offset = width * multiplier
            rects = ax4.bar(x + offset, measurement, width, label=attr)
            ax4.bar_label(rects, fmt='%.3f', padding=3, fontsize=8)
            multiplier += 1
        
        ax4.set_title('ç™¾åˆ†ä½æ•°æ¯”è¾ƒ')
        ax4.set_ylabel('æ—¶é—´ (ms)')
        ax4.set_xticks(x + width, operators)
        ax4.legend(loc='upper left')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        
        plt.show()
